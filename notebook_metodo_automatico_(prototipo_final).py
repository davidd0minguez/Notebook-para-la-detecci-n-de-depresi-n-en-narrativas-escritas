# -*- coding: utf-8 -*-
"""Notebook: Metodo automatico (Prototipo_final).ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1KT7NyTINwY_GTf0AFl7ehDivD41xQhyx

###LIBRERIAS

---
"""

#@title <- Activa las librerias
import spacy
import requests
from bs4 import BeautifulSoup

!pip install deplacy
!python -m spacy download es_core_news_sm
import pkg_resources,imp
imp.reload(pkg_resources)
import spacy
nlp=spacy.load("es_core_news_sm")
from collections import Counter
#!pip install textacy
#import textacy

import numpy as np
import matplotlib.pyplot as plt
!pip install monkeylearn

#@title <-Librerias
!pip install nltk
!pip install wordcloud
!pip install tweepy

#@title <- Librerias de PLN
import re 
import tweepy 
import nltk
import pandas as pd
import matplotlib.pyplot as plt
from wordcloud import WordCloud,STOPWORDS
nltk.download('punkt')   
nltk.download('stopwords')
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from nltk.stem import PorterStemmer
from tweepy import OAuthHandler 
from textblob import TextBlob
import pandas as pd

#@title <- Cargue librerías
!pip3 install tweepy

#@title <- Librerias de Excel
!pip install openpyxl

#@title <- Librerias de analisis y PLN

import spacy
import requests
from bs4 import BeautifulSoup

!pip install deplacy
!python -m spacy download es_core_news_sm
import pkg_resources,imp
imp.reload(pkg_resources)
import spacy
nlp=spacy.load("es_core_news_sm")
from collections import Counter
#!pip install textacy
#import textacy

import numpy as np
import matplotlib.pyplot as plt
!pip install monkeylearn
import openpyxl

"""###EXTRAER INFORMACION DESDE DRIVE

---


"""

#@title <- Conecta con Drive
from google.colab import drive
drive.mount('/content/drive')

"""### CARGA DEL ARCHIVO DE RESPUESTAS

---
"""

#@title <- Sube el archivo descargarste de Google Drive (Respuestas dadas)
from google.colab import files
files.upload()

#@title <- Valida que el archivo que cargaste se llama "Detección de sintomatologia emocional (Respuestas)"
import openpyxl
doc = openpyxl.load_workbook ("/content/Detección de sintomatologia emocional (Respuestas).xlsx")
hoja = doc.get_sheet_by_name("Respuestas de formulario 1")
preguntas = hoja.delete_rows(1)

"""###ANALISIS A EL CONSULTANTE

---
"""

#@title <- Agrege ID de la consulta del consultante
id = int(input("Ingrese el ID de la consulta del paciente = "))

#@title <-Analisis de respuestas con lenguaje natural y repeticion de palabras
print("Análisis de respuestas, del paciente ID ",id,"\n")
################### Librerias especificas ###################
import spacy
import unicodedata
import string


#from spacy.lang.es.examples import sentences 
#import unicodedata
#nlp=spacy.load("es_core_news_sm")
################### Arreglos ###################
respuestas=[]
respuestasprev=[]
palabras=[]
respuestasnone=[]
################### Información correspondiente al usuario desde hoja de calculo ###################
celdainicial = "BD{0}"
celdainicialF = celdainicial.format(id)

celdafinal = "BI{0}"
celdafinalF = celdafinal.format(id)

multiple_cells = hoja[celdainicialF:celdafinalF]
for row in multiple_cells:
    for cell in row:
        respuestasnone.append(cell.value)
################### Limpieza de texto ###################
respuestasprev = list(filter(None, respuestasnone))
Respuestascompletas= str(respuestasprev)
#Respuestascompletas = " ".join(respuestas)

output_clean = Respuestascompletas.replace('\n', ' ')
output_clean2 = output_clean.replace('.', ' ')
output_clean3 = output_clean2.replace(',', ' ')
output_clean3 = output_clean2.replace("''","")
output_clean3 = output_clean2.replace("'","")
output_clean3 = output_clean2.replace(",","")
#output_clean30 = output_clean2.replace('[', '')
#output_clean31 = output_clean2.replace(']', '')
output_clean32 = ''.join([c for c in output_clean3 if ord(c) > 31 or ord(c) == 9])

trans_tab = dict.fromkeys(map(ord, u'\u0301\u0308'), None)#Tildes
output_clean4 = unicodedata.normalize('NFKC', unicodedata.normalize('NFKD', output_clean32).translate(trans_tab))#Formato de texto
output_clean5 = output_clean4.title() 
doc = nlp(output_clean5)
palabras = output_clean5.split()
################### Conteo de palabras ###################
contador = Counter(palabras)
buscar = 'depresion' 
import re
regex="("+".*?".join(buscar) + ")"
lista1Cadena=" ".join(palabras)
encontrados = re.finditer(regex, lista1Cadena, re.MULTILINE)
cantidad=len([matchNum for matchNum, match in enumerate(encontrados, start=1)])
print("____________________________________________")
print("___________YA SE PUEDEN EJECUTAR ___________")
print("__________LOS SIGUIENTES ANÁLISIS __________")
print("____________________________________________")
print("")

#@title <- Sujetos detectados de las repuestas dadas por el consultante.
########################################################################SUJETOS###################################################################################################
fig = plt.figure(figsize=(10,5))
ax = fig.gca()

Esencial = [chunk.text for chunk in doc.noun_chunks]
contador = Counter(Esencial)

labels, values = zip(*contador.most_common(20))

indexes = np.arange(len(labels))
width = 1

ax.bar(indexes, values, 0.5)
ax.set_xticks(indexes)
ax.set_xticklabels( labels, rotation=90, fontsize=20)
ax.set_title("Sujetos detectados de las repuestas\ndadas por el paciente en la consulta.", fontsize=20)
plt.show()
####################################################################################################################################################################################

#@title <- Adjetivos detectados de las repuestas dadas por el consultante.
##################################################################Adjetivos###############################################################################################
fig = plt.figure(figsize=(10,5))
ax = fig.gca()

ADJ_F = [] 

for token in doc:
  if token.pos_ == "ADJ":
     ADJ_F.append(token.text)

#print("Adjetivos = ",ADJ_F)
contador = Counter(ADJ_F)

labels, values = zip(*contador.most_common(20))

indexes = np.arange(len(labels))
width = 1

ax.bar(indexes, values, 0.5)
ax.set_xticks(indexes)
ax.set_xticklabels( labels, rotation=90, fontsize=20)
ax.set_title("Adjetivos detectados de las repuestas\ndadas por el paciente en la consulta.", fontsize=20)
plt.show()
####################################################################################################################################################################################

#@title <- Verbos detectados de las repuestas dadas por el consultante.
##################################################################Verbos###############################################################################################
fig = plt.figure(figsize=(10,5))
ax = fig.gca()

VERB_F = [] 

for token in doc:
  if token.pos_ == "VERB":
     VERB_F.append(token.text)

#print("Verbos = ",VERB_F)
contador = Counter(VERB_F)

labels, values = zip(*contador.most_common(20))

indexes = np.arange(len(labels))
width = 1

ax.bar(indexes, values, 0.5)
ax.set_xticks(indexes)
ax.set_xticklabels( labels, rotation=90, fontsize=20)
ax.set_title("Verbos detectados de las repuestas\ndadas por el paciente en la consulta.", fontsize=20)
plt.show()
####################################################################################################################################################################################

#@title <- Palabras mas usadas por el consultante.
################################POR PALABRAS MAS COMUNES############################
fig = plt.figure(figsize=(10,5))
ax = fig.gca()

contador = Counter(palabras)

labels, values = zip(*contador.most_common(20))

indexes = np.arange(len(labels))
width = 1

ax.bar(indexes, values, 0.5)
ax.set_xticks(indexes)
ax.set_xticklabels( labels, rotation=90, fontsize=20)
ax.set_title("Las palabras más frecuentemente usadas\npor el paciente", fontsize=20)
plt.show()
############################################################

#@title <- Cuantas veces se encontro el concepto de Depresion:

print("El concepto de 'Depresion' se encontro {} vez en las respuestas".format(cantidad))
############################################################

#@title <- Veces se encontro el concepto de:

buscar = str(input("Que concepto desea buscar su frecuencia de uso = "))
veces = 0

buscarM = buscar.title() 

veces = palabras.count(buscarM)

if buscarM in palabras:
  print("El concepto de",buscar,"se encuentra en las respuestas dada por el usuario,",veces,"veces.")
else:
  print("El concepto de",buscar,"NO se encuentra en las respuestas dada por el usuario.")

"""###RELACION CON LA ONTOLOGÌA

---
"""

#@title <- Carga la Ontologia
Animo=["Perezoso","Desalentada","Desalentadas","Decaído","Desmotivado","Desalentado","Desalentados","Desalentó","No Tengo Ganas","No Quiero","Estoy Cansado","Me Canso","No Tengo Alientos","Estoy Cansada","Pena","Desaliento","Apenado","Penoso","Penosa","Apenada","Deprimido","Desanimado","Decaído","Melancólico","Me Hundo","Humillado","Depresivo","Depresión","Me Arrepiento","No Quería Hacerlo","Debía Hacerlo","Me Siento Mal","Estoy Deprimido","Me Deprime","Estoy Fatal","No Puedo Más","Me Duele","Dolor","Melancolía","Afligido","Afligida","Triste","Desconsolado","Sad","Emo","Nada","Tengo Un Vacío","Me Ahogo","Oscuridad","Estoy En El Fondo","Cayendo","Caída","Atrapado","Atrapada","No Tengo A Nadie","Tristeza","Abatimiento","Hábito","Igual","Hábitos","No Quiero Hacer","Perdida","Perdido","Pérdida","Túnel","Frágil","Miedo","Intranquilo","Desconsolado  ","Desilusionado ","Decepcionado","Desinteresado","Fatigado","Cansado","Intranquilo","Quebrantado","Ataque","Lloro","Libido","Lagrimoso","Falta","Sin Amor","Nada Me Sale Bien","Indigno","Me Siento Mal","Soledad","Estoy Solo","Solito","Me Siento ","Miserable","Nada Motivado","Desesperado","Desesperanzado","Reducido","Háptico","No Estoy Al Ciento Por Ciento","No Encuentro Calma","Muerte","Suicida","Plan Estructurado","Plan","Planeacion","Todo Es Mi Culpa","Soy El Culpable","Soy La Culpable","Por Mi Culpa","Soy Una Vergüenza","No Me Interesa","No Me Motiva","No Hay Nada","Abrumado","Desesperado","Desesperada","Abrumada","Me Hundo","No Soy Capaz","No Puedo","Todo Falla","No Hay Nada","Inhabilidad","Soy Invisible","Encerrado","Encerrada","Es Una Tortura","Paralizado","Desesperanza","Es Un Infierno","Peso En El Pecho","Peso","No Puedo Luchar ","Me Arrastra","Me Odio","Quiero Morir","Me Voy A Matar","No Quiero Despertar","No Quiero Seguir","Ojalá Me Muera","Culpa","Desinterés","Llorar","Perdida De Confianza","Solo","Miserable","Sensible","Pensamiento Recurrente","Desilusionado","Desesperanza","Desesperanzado","Decepcionado ","Fracasado","Contrariado","Desanimado ","Susceptible","Desaliento","Desalentado","Soló","Lloró","Desesperado","Melancólico","Insignificante","Triste","Vacío ","Irritable","Afligido ","Sad","Apagado","Down ","Nostálgico ","Decaído","Sensible","Deplorable ","Abatido","Inestable"]

Apatia=["No Tiro","No Me Dan Ganas","Pasivo","No Siento ","Nada De Sexo","No Quiero Nada","Dejadez","Dejado","Vacío","Abandono","Desinteresado","Desánimo","Desganado","Desaliento","Descuidado","No Quiero Hacer Nada","No Puedo","Me Pesa","Me Da Igual","Cansado","No Me Hace Nada","No Siento Lo Mismo","No Tiene Sentido","No Disfruto","No Me Interesa","Me Aislo","Es Una Agonía","Sin Ganas","No Siento Deseo","Indiferente","Nada Motivado","Desesperado","Desesperanzado","Reducido","Apatico","No Estoy Al Ciento Por Ciento","No Encuentro Calma","No Pasa","Jamás","Tampoco","Es Común Que No","Nadie","Ni Siquiera","Nada","No Quiero Hacer","Sin Amor","Amor Propio","No Puedo","Difícil","Me Queda Grande","Impotente","Inferioridad","Nada Me Sale Bien","Todo Me Sale Mal","Sensible","Nunca","Ninguno","Abatimiento","Inutilidad","Incapacidad","Aislado","Insatisfecho","Solo","Vacío","Ausente","Melancólico","Aislamiento","Ausente","Inferioridad","Desalentada","Desalentadas","Decaído","Desmotivado","Desalentado","Desalentados","Desalentó","Aislamiento","Desaliento","Desinteresado","Sin Interés ","Apático","Desmotivado","Invalidante","Insensible","Desapegado","Desagradecido","Despreocupado"]

Alimentacion=["Indiferente","Yo","Apetito","Desinterés","Emoción","Abatimiento","Dejadez","Dejado","Vacío","Abandono","Desinteresado","Desánimo","Deganado","Desaliento","Descuidado","Todo Yo","Siempre Es Igual","Siempre A Mi","Soy","No Como","Poco Como","Hambre","Vacío","Inapetente","No Tengo Hambre","Nada De Apetito","Desinteresado","Fatigado","Cansado","Intranquilo","Nada","No Quiero Comer","Impulso","Glotón","Goloso","Sediento","Glotona","Hastiado/A","Inapetente","Fastidiado","Repugnante","Vomito","Insasiable","Devoro","Lleno","Repleto","Hambriento","Antojado","Como Mucho","Ingiero","Pesadez ","Atracón"]

Descanso=["Débil","Débiles","Debilité","Raquítico","Debilitado","Enclenque","Flojo","Apagado","Debilitada","Mamado","Debilitados","Floja","Debilitadas","Desalentada","Desalentadas","Decaído","Desmotivado","Desalentado","Desalentados","Desalentó","Debilidad","Desaliento","Agitacion","Falta De Energia","Igual","Siempre","Lo Mismo","Dia A Dia","Rutinario","Todos Los Días","Cansado","Extenuado","Mamado","Harto","Igual","Hábitos","No Duermo","Duermo Mucho","Levanto Tarde","No Duermo","Insomnio","Desvelado","Desvelo","Sencillo","Sensible","Cotidiano","Extenuación","Hábito","Sueño","Susceptible","Nada Motivado","Desesperado","Desesperanzado","Reducido","Háptico","No Estoy Al Ciento Por Ciento","No Encuentro Calma","Sensible","Agotado","Sin Ganas","Cansado","Cansada","Exhausto","Exhausta","Somnoliento","Molido","Rendido ","Saturado","Debilitado","Pesado","Desvanecido","Acabado","Cargado","Cansado"]

Actividad_fisica=["Desaliento","Frustración","Trauma","Abatimiento","Extenuación","Incapacidad","Agitado","Débil","Débiles","Debilité","Raquítico","Debilitado","Enclenque","Flojo","Apagado","Debilitada","Mamado","Debilitados","Floja","Debilitadas","Desalentada","Desalentadas","Decaído","Desmotivado","Desalentado","Desalentados","Desalentó","Frustrado","Fracasado","Fiasco","Chasco","Traumatizado","Afectado","Marcado","Encadenado","Enyugado","Atado","Nada","No Quiero Hacer","Cansado","Extenuado","Mamado","Harto","No Puedo","Difícil","Me Queda Grande","Impotente","Nerviosismo","Ansioso","Malo","Malestar","Inmunda","Inquieto","Preocupado","Preocupación","Debilidad","Lento","Inquieto","Pasivo","Pausado","Fatigado","Acelerado","Inerte","Activo","Flojo","Tonto","Retardado","Revuelto","Pasmado","Callado"]

Desaliento=["Ausente","Ausentes","Ausentado","Ausencia","Alejado","Lejano","Ausentada","Ausentó","Ausentará","Ausente","Débil","Débiles","Debilité","Raquítico","Debilitado","Enclenque","Flojo","Apagado","Debilitada","Mamado","Debilidad","Floja","Desalentada","Decaído","Desmotivado","Desalentado","Desaliento","Desalentó","Errores","Embarrarla","Cagarla","Tirarla","Surrarla","Miedo Al Fracaso","Derrotado","Abatido","Cabizbajo","Desanimado","Melancólico","Decaído","Derrumbado","Desmoronado","Abatimiento","Igual","Siempre","Repetitivo","Tedioso","Rutinario","Rutina ","Cansado","Extenuado","Mamado","Harto","Monotonía","Cotidiano","Extenuación","Hábito","Hábitos","Hábito","Desvelo","Somnolente","Adormecido","Somnolento","Insomnio","Desvelado","Desvelo","Sencillo","Sensible","Afligido","Amargura","Amargado","Enfadado","Molesto","Desinteresado","Fatigado","Cansado","Intranquilo","Sueño","Susceptible","Aflicción","Desinterés","Apático","Indiferente","Aislado","Insatisfecho","Solo","Vacío","Ausente","Melancólico","Negativo","Distraído","Problemas","Disperso","Elevado","Aislamiento","Atención","Nerviosismo","Quebrantado","Inmunda","Inquieto","Inunda","Desfallecido","Nervioso","Cargado","Aburrido","Agobiado","Sofocado","Inatento","Desatento","Aburrimiento","Agobiado","Pesado","Agotado","Debil ","Agotada","Dejado","Decaido ","Decaida","Fragil","Molido","Desmayado","Asfixiado","Agobiado","Desganado ","Distante","Enfermo","Parsimonioso","Sueño"]

Cargo_de_conciencia=["Culpable","Culpado","Culpada","Culpa","Culpé","Culpar","Culpables","Errores","Embarrarla","Cagarla","Tirarla","Surrarla","Frustrado","Fracasado","Fiasco","Chasco","Apenado","Penoso","Penosa","Apenada","Deprimido","Desanimado","Decaído","Melancólico","Hundido","Depresivo","Lamentar","Lamentó","Lamenté","Lamento","Afligido","Afligida","Arrepentido","Arrepentida","Arrepentidos","Arrepentidas","Arrepintieron","Arrepentí","Desmoronado","No Quiero Hacer","Confuso","Confundo","Incomprender","Perdido","Desvelo","Somnolente","Adormecido","Somnolento","Insomnio","Desvelado","Desvelo","Vulnerable","Sensible","Frágil","Afligido","Amargura","Amargado","Enfadado","Molesto","Miedo","Intranquilo","Desconsolado  ","Desilusionado ","Decepcionado","Intranquilo","Quebrantado","Ataque","Lloro","Libido","Lagrimoso","Aislado","Insatisfecho","Solo","Vacío","Ausente","Melancólico","Negativo","Autocriticar","Indigno","Inmunda","Preocupado","Preocupación","Dudo","Dudar","Incierto","Dejadez","Dejado","Vacío","Abandono","Desinteresado","Desánimo","Desganado","Desaliento","Descuidado","Futuro","Irresponsabilidad","Irresponsable","Responsable","Responsabilidad","Incumplido","Aplazar","Procrastino","Evadir","Evitar","Rumiar","Rumiación","Incapz","Inútil","Torpe","Inepto","Incompetente","Inhábil","Ignorante","Insuficiente","Inseguro","Remordimiento","Acusado","Derrotado","Derrumbado","Desorrientado","Cabizbajo","Desanimado","Melancólico","Decaído","Irritable","Culpabilidad","Miedo Al Fracaso","Frustración","Pena","Tristeza","Acusación","Abatimiento","Confusión","Sueño","Susceptible","Aflicción","Angustia","Irritable","Llorar","Aislamiento","Autoestima","Nervioso","Duda","Indiferente","Inservible","Incompetente","Fracasado","Incapaz","Desastroso","Culpable","Desilusionado","Irresponsable","Preocupado","Preocupada","Inseguro","Inutil","Insuficiente","Quejambroso","Reproche","Inferoridad","Inferior ","Incomprendido ","Mediocre","Decepcionado","Decepcionada","Tonto","Vago"]

Atencion_dispersa=["Ausente","Ausentes","Ausentado","Ausentados","Ausentada","Ausentadas","Ausentó","Ausentará","Ausente","Ausencia","Errores","Embarrarla","Cagarla","Tirarla","Cerrarla","Frustrado","Fracasado","Fiasco","Chasco","Frustración","Disperso","Traumatizado","Afectado","Marcado","Encadenado","Enyugado","Atado","Trauma","No Duermo","Duermo Mucho","Levanto Tarde","No Duermo","Insomnio","Desvelado","Sueño","Distraído","Problemas","No Concentrado","Dudo","Dudar","Incierto","Cargado","Duda","Confuso","Confundo","No Entiendo","Perdido","Bruto","Confusión","Confundido","Confundida","Distraído ","Desatento","Olvidadizo ","Incoherente","Descuidado","Embolatado","Desconcentrado","Despistado","Desordenado","Desmemoriado ","Equivocado","Desorientado","Descoordinado","Torpe","Ido","Englobado"]

Ideas_suicidas=["Desvelo","Sensible","Frágil","Miedo","Intranquilo","Desconsolado  ","Desilusionado ","Decepcionado","Susceptible","Sosobra","Aislado","Insatisfecho","Solo","Vacío","Ausente","Melancólico","Negativo","Aislamiento","Me Siento ","Miserable","Nada Motivado","Desesperado","Desesperanzado","Reducido","Apatico","No Estoy Al Ciento Por Ciento","No Encuentro Calma","Sensible","Tristeza","Melancolía","Afligido","Afligida","Triste","Desconsolado","Sad","Emo","Estorbo","Basura","Tortura","Dolor","Problema","Muero","Morir","Muerte","Desaparecer","Harto","Suicidarme","Suicidio","Matarme","Desaparecer","Down","Sin Futuro","Morir ","Perdido","Destruido","Aliviar Dolor","Golpearme","Sin Esperanza","Dolor","Muerte ","Llegar Al Final","Colgarme","Lanzarme","Derrotado","Rendido","Impotente","Desbordado ","Desesperado","Desesperada","Quebrantado","Sufrimiento","Hacerme Daño"]

#@title <- Relaciona la Ontologia de sintomatologia depresvia, con las diferentes respuestas generadas por el paciente.
ramaf = []

print("Palabras relacionadas con la sintomatologia: Animo")
rama = []
palabras_estudiar=set(palabras)
Animo=set(Animo)
Animo= palabras_estudiar & Animo
if len(Animo) > 0 :
    print("Hay {} elementos coincidentes".format(len(Animo)))
    print(Animo)
    rama.append("Animo")
    ramaf.append("Animo")
else:
    print("No hay repeticiones")
print("")
######

print("Palabras relacionadas con la sintomatologia: Apatia")
rama = []
palabras_estudiar=set(palabras)
Apatia=set(Apatia)
Apatia= palabras_estudiar & Apatia
if len(Apatia) > 0 :
    print("Hay {} elementos coincidentes".format(len(Apatia)))
    print(Apatia)
    rama.append("Apatia")
    ramaf.append("Apatia")
else:
    print("No hay repeticiones")
print("")
######

print("Palabras relacionadas con la sintomatologia: Alimentacion")
rama = []
palabras_estudiar=set(palabras)
Alimentacion=set(Alimentacion)
Alimentacion= palabras_estudiar & Alimentacion
if len(Alimentacion) > 0 :
    print("Hay {} elementos coincidentes".format(len(Alimentacion)))
    print(Alimentacion)
    rama.append("Alimentacion")
    ramaf.append("Alimentacion")
else:
    print("No hay repeticiones")
print("")   
######

print("Palabras relacionadas con la sintomatologia: Descanso")
rama = []
palabras_estudiar=set(palabras)
Descanso=set(Descanso)
Descanso= palabras_estudiar & Descanso
if len(Descanso) > 0 :
    print("Hay {} elementos coincidentes".format(len(Descanso)))
    print(Descanso)
    rama.append("Descanso")
    ramaf.append("Descanso")
else:
    print("No hay repeticiones")
print("")   
######

print("Palabras relacionadas con la sintomatologia: Actividad_fisica")
rama = []
palabras_estudiar=set(palabras)
Actividad_fisica=set(Actividad_fisica)
Actividad_fisica= palabras_estudiar & Actividad_fisica
if len(Actividad_fisica) > 0 :
    print("Hay {} elementos coincidentes".format(len(Actividad_fisica)))
    print(Actividad_fisica)
    rama.append("Actividad_fisica")
    ramaf.append("Actividad_fisica")
else:
    print("No hay repeticiones")
print("")        
######

print("Palabras relacionadas con la sintomatologia: Desaliento")
rama = []
palabras_estudiar=set(palabras)
Desaliento=set(Desaliento)
Desaliento= palabras_estudiar & Desaliento
if len(Desaliento) > 0 :
    print("Hay {} elementos coincidentes".format(len(Desaliento)))
    print(Desaliento)
    rama.append("Desaliento")
    ramaf.append("Desaliento")
else:
    print("No hay repeticiones")
print("")            
######

print("Palabras relacionadas con la sintomatologia: Cargo_de_conciencia")
rama = []
palabras_estudiar=set(palabras)
Cargo_de_conciencia=set(Cargo_de_conciencia)
Cargo_de_conciencia = palabras_estudiar & Cargo_de_conciencia
if len(Cargo_de_conciencia) > 0 :
    print("Hay {} elementos coincidentes".format(len(Cargo_de_conciencia)))
    print(Cargo_de_conciencia)
    rama.append("Cargo_de_conciencia")
    ramaf.append("Cargo_de_conciencia")
else:
    print("No hay repeticiones")
print("")                
######

print("Palabras relacionadas con la sintomatologia: Atencion_dispersa")
rama = []
palabras_estudiar=set(palabras)
Atencion_dispersa=set(Atencion_dispersa)
Atencion_dispersa = palabras_estudiar & Atencion_dispersa
if len(Atencion_dispersa) > 0 :
    print("Hay {} elementos coincidentes".format(len(Atencion_dispersa)))
    print(Atencion_dispersa)
    rama.append("Atencion_dispersa")
    ramaf.append("Atencion_dispersa")
else:
    print("No hay repeticiones")
print("")                    
######

print("Palabras relacionadas con la sintomatologia: Ideas_suicidas")
rama = []
palabras_estudiar=set(palabras)
Ideas_suicidas=set(Ideas_suicidas)
Ideas_suicidas = palabras_estudiar & Ideas_suicidas
if len(Ideas_suicidas) > 0 :
    print("Hay {} elementos coincidentes".format(len(Ideas_suicidas)))
    print(Ideas_suicidas)
    rama.append("Ideas_suicidas")
    ramaf.append("Ideas_suicidas")
else:
    print("No hay repeticiones")

"""###APRENDIZAJE DE MAQUINA

---

**Analisis por oraciones**
"""

#@title <- Texto del cuestionario separado por comas

output_clean.replace(".",",")
output_clean.replace(";",",")
texto_x_comas = output_clean.split(',')

from itertools import islice

def group_elements(lst, chunk_size):
    lst = iter(lst)
    return iter(lambda: tuple(islice(lst, chunk_size)), ())

for new_list in group_elements(texto_x_comas , 1):
    print(new_list)

#@title <- Cargar dataset de entrenamiento del modelo

import pandas as pd
import io 
from google.colab import files   
  
uploaded = files.upload()

#@title <- Verificación / presentación dataset de entrenamiento

df = pd.read_csv('Dataset aprendizaje metodo automatico.csv') 
df.head()

#@title <- Metrica de desempeño: Accuracy Score

from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.naive_bayes import MultinomialNB

DV = "Nombre sintoma"
X = df.drop([DV], axis = 1) 
Y = df[DV]

#Train 75%, 25% testing - 70 30 - 80 20 - 75 25
X_train, X_test, Y_train, Y_test = train_test_split(X,Y, test_size=0.25)

count_vect = CountVectorizer(max_features = 5000) 

X_train_counts = count_vect.fit_transform(X_train['Oracion'])

X_test = count_vect.transform(X_test['Oracion'])

from sklearn.metrics import accuracy_score

Naive = MultinomialNB() 
Naive.fit(X_train_counts, Y_train)
predictions_NB = Naive.predict(X_test)
print("Accuracy Score:", accuracy_score(predictions_NB, Y_test)*100,"%")

#@title <- Metrica de desempeño: Matriz de confusión

from sklearn.metrics import confusion_matrix
print("Matriz de confusión:")
confusion_matrix(predictions_NB, Y_test)

#@title <- Metrica de desempeño: Relacion a ontologia de oraciones

output_clean.replace(".",",")
output_clean.replace(";",",")
texto_x_comas = output_clean.split(',')
clasif_general = []

def group_elements(lista):
    lista_vec = count_vect.transform(lista) 
    predict_lista = Naive.predict(lista_vec) 
    return predict_lista

for predict_lista in group_elements(texto_x_comas):
    clasif_general.append(predict_lista)
    #print(predict_lista)

lista = clasif_general

repeticiones = {}
for n in lista:
  if n in repeticiones :
    repeticiones[n] += 1
  else:
    repeticiones[n] = 0

resultado={}
for clave in repeticiones:  
  valor=repeticiones[clave]
  if valor != 0:
    resultado[clave] = valor
print("Catogorias + cantidad de identificaciones: ",resultado)
print("Cantidad de categorias identificadas: ",len(resultado))

#@title <- Reporte general de Metricas de desempeño

from sklearn.metrics import classification_report

Naive = MultinomialNB() 
Naive.fit(X_train_counts, Y_train)
predictions_NB = Naive.predict(X_test)
print("classification_report")
print(classification_report(predictions_NB, Y_test))

"""###INFORME DE RESULTADOS

---
"""

#@title <- Analisis generado con aprendizaje de maquina
from monkeylearn import MonkeyLearn
celdaML = "L{0}"
celdaMLF = celdaML.format(id)

respuestasML = []

multiple_cells = hoja[celdaMLF:celdaMLF]
for row in multiple_cells:
    for cell in row:
        respuestasML.append(cell.value)
#print(respuestasML)
ml = MonkeyLearn('6443587718d4ce3259255d93d0cbb4b61d4ab3e0')
model_id = 'cl_prjHdwSn'
result = ml.classifiers.classify(model_id, respuestasML)
modelo=result.body
StrModelo = "".join(map(str, modelo))
StrModelo2 = "".join(StrModelo)
Modelo3 = StrModelo2.split()
#print("POS",Modelo3.index("[{'tag_name':"))
POS1 = Modelo3.index("[{'tag_name':")
ModeloClas= (Modelo3[POS1:POS1+4])

StrClasificacion = "".join(map(str, ModeloClas))
StrClasificacion2 = "".join(StrClasificacion)

StrClasificacion2=StrClasificacion2.replace("[","")
StrClasificacion2=StrClasificacion2.replace("'","")
StrClasificacion2=StrClasificacion2.replace("{","")
StrClasificacion2=StrClasificacion2.replace(","," ")
StrClasificacion2=StrClasificacion2.replace("tag_name","Clasificacion_ML")
print(StrClasificacion2)
#print("POS",Modelo3.index("'confidence':"))
#POS2 = Modelo3.index("'confidence':")
#probabilidad_clas=(Modelo3[POS2:POS2+2])
probabilidad_clas=(Modelo3[POS1+5:POS1+6])
StrProbabilidad = "".join(map(str, probabilidad_clas))
StrProbabilidad2 = "".join(StrProbabilidad)
StrProbabilidad2=StrProbabilidad2.replace("[","")
StrProbabilidad2=StrProbabilidad2.replace("'","")
StrProbabilidad2=StrProbabilidad2.replace("{","")
StrProbabilidad2=StrProbabilidad2.replace("]","")
StrProbabilidad2=StrProbabilidad2.replace("}","")
StrProbabilidad2=StrProbabilidad2.replace(","," ")

#############################################

POS2 = Modelo3.index("'confidence':")
valor= (Modelo3[POS2:POS2+5])
valor = "".join(map(str, valor))
valor = "".join(valor)
valor=valor.replace("[","")
valor=valor.replace("'","")
valor=valor.replace("{","")
valor=valor.replace("]","")
valor=valor.replace("}","")
StrProbabilidad2=valor.replace(","," ")
print("Probabilidad_Clasificacion: ",StrProbabilidad2[13],StrProbabilidad2[14],".",StrProbabilidad2[15],"%")

"""**Aca iria la relacion del texto por oraciones con la Ontologia de oraciones**"""

#@title <- Genera informe de resultados
#cargar informe

contador = Counter(palabras)

mas = contador.most_common(20)

Generar_informe=open("Informe.txt","w") 
Generar_informe.close()

informe = ("Este es el informe generado para el paciente con el ID : {0} \n"
           "\n"
           "El escribió estas palabras en sus respuestas: {1} \n"
           "\n"
           "Al relacionar estas palabras con la Ontología encontramos que presenta correlación con: \n"
           "La sintomatologia de : {2}"
           "\n"
           "Estos son los conceptos identifacados por rama especifica:"
           "\n"
           "Animo: {3}"
           "\n"
           "Apatia: {4} "
           "\n"
           "Alimentacion: {5}"
           "\n"
           "Descanso: {6}"
           "\n"
           "Actividad_fisica: {7}"
           "\n"
           "Desaliento: {8}"
           "\n"
           "Cargo_de_conciencia: {9}"
           "\n"
           "Atencion_dispersa: {10}"
           "\n"
           "Ideas_suicidas: {11}"
           "\n"
           "Si anteriormente no se ve ninguna relación es porque no se encontraron correlaciones.\n"
           "\n"
           "Las palabras mas significativas dadas por el consultante son: \n"
           "{12}"
           "\n"
           "\nLos adjetivos que este utilizo fueron: \n"
           "{13}"
           "\n"
           "\nLos verbos que este utilizo fueron: \n"
           "{14}"
           "\n"
           "\nLas palabras mas usadas por el consultante son: \n"
           "{15}"
           "\n"
           "\n"
           "\nLa clasificacion dada por el modelo de aprendizaje fue : \n"
           "{16}"
           "\n"
           "\n"
           "\nY su probabilidad es : \n"
           "{17}"
           "\n"
           "\n"
           "\nSi en alguno de los anteriores, te genero vacío, es porque el consultante no utilizo ningún concepto\n"
           "relacionado a esa área especifica.\n"
           )

informeP = informe.format(id,palabras,ramaf,Animo,Apatia,Alimentacion,Descanso,Actividad_fisica,Desaliento,Cargo_de_conciencia,Atencion_dispersa,Ideas_suicidas,Esencial,ADJ_F,VERB_F,mas,StrClasificacion2,StrProbabilidad2)
informeF = "".join(informeP)

Cargar_informe=open("Informe.txt","a")
Cargar_informe.write(informeF)
Cargar_informe.close()

"""**Analisis de Twitter**"""

#@title <- Posible cuenta de Twitter

cuenta=[]
CuentaTwitter=[]

celdainicial = "R{0}"
celdainicialF = celdainicial.format(id)

celdafinal = "R{0}"
celdafinalF = celdafinal.format(id)

multiple_cells = hoja[celdainicialF:celdafinalF]
for row in multiple_cells:
    for cell in row:
        cuenta.append(cell.value)
################### Limpieza de texto ###################

CuentaTwitter = str(cuenta)
CuentaTwitter = CuentaTwitter.replace("[","")
CuentaTwitter = CuentaTwitter.replace("'","")
CuentaTwitter = CuentaTwitter.replace("'","")
Cuenta = CuentaTwitter.replace("]","")
print("@",Cuenta)

#@title <- Ingrese el usuario a consultar (Sin el "@")= 
usuario = input("Ingrese el usuario a consultar = ")

#@title <-Librerias
!pip install nltk
!pip install wordcloud
!pip install tweepy

#@title <- Librerias de PLN
import re 
import tweepy 
import nltk
import pandas as pd
import matplotlib.pyplot as plt
from wordcloud import WordCloud,STOPWORDS
nltk.download('punkt')   
nltk.download('stopwords')
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from nltk.stem import PorterStemmer
from tweepy import OAuthHandler 
from textblob import TextBlob

#@title <- Llaves de twitter

def connect():
  consumer_key = "MqqpoANH1PXbpl8Tq5Ys8WdP4"
  consumer_secret = 'vdjzTDWlgC13t1Cdl0j8gD3mQATD9kGbRC5mgWZxuqu5sols25'
  access_token = '1298016354516140037-b4HAgP18N7fXAwJdjcebvUrlcTcGiD'
  access_token_secret = '7tPLwRlJry3OeW1tN9XRblEE0pHqCmE3XfBIQtn7a6zm0'

  try:
    auth = OAuthHandler(consumer_key, consumer_secret)
    auth.set_access_token(access_token, access_token_secret)
    api = tweepy.API(auth)
    return api
  except:
    print("Error")
    exit(1)

#@title <- Limpieza de los tweets
def cleanText(text):
  text = text.lower()
  # Removes all mentions (@username) from the tweet since it is of no use to us
  text = re.sub(r'(@[A-Za-z0-9_]+)', '', text)
    
  # Removes any link in the text
  text = re.sub('http://\S+|https://\S+', '', text)

  # Only considers the part of the string with char between a to z or digits and whitespace characters
  # Basically removes punctuation
  text = re.sub(r'[^\w\s]', '', text)

  # Removes stop words that have no use in sentiment analysis 
  text_tokens = word_tokenize(text)
  text = [word for word in text_tokens if not word in stopwords.words()]

  text = ' '.join(text)
  return text

#@title <- Tokenizar palabras
def stem(text):
  # This function is used to stem the given sentence
  porter = PorterStemmer()
  token_words = word_tokenize(text)
  stem_sentence = []
  for word in token_words:
    stem_sentence.append(porter.stem(word))
  return " ".join(stem_sentence)

#@title <- Clasifiacion de los tweets

def sentiment(cleaned_text):
  # Returns the sentiment based on the polarity of the input TextBlob object
  if cleaned_text.sentiment.polarity > 0:
    return 'positive'
  elif cleaned_text.sentiment.polarity < 0:
    return 'negative'
  else:
    return 'neutral'

#@title <- PLN a los tweets

def fetch_tweets(query, count = 50):
  api = connect() # Gets the tweepy API object
  tweets = [] # Empty list that stores all the tweets

  try:
    # Fetches the tweets using the api
    fetched_data = api.search(q = query + ' -filter:retweets', 
count = count)
    for tweet in fetched_data:
      txt = tweet.text
      clean_txt = cleanText(txt) # Cleans the tweet
      stem_txt = TextBlob(stem(clean_txt)) # Stems the tweet
      sent = sentiment(stem_txt) # Gets the sentiment from the tweet
      tweets.append((txt, clean_txt, sent))
    return tweets
  except tweepy.TweepError as e:
    print("Error : " + str(e))
    exit(1)

#@title <- Categorizacion (Positivas / Neutral /Negativa)

tweets = fetch_tweets(query = usuario, count = 100)
# Converting the list into a pandas Dataframe
df = pd.DataFrame(tweets, columns= ['tweets', 'clean_tweets','sentiment'])

# Dropping the duplicate values just in case there are some tweets that are copied and then stores the data in a csv file
df = df.drop_duplicates(subset='clean_tweets')
df.to_csv('data.csv', index= False)
ptweets = df[df['sentiment'] == 'positive']
p_perc = 100 * len(ptweets)/len(tweets)
ntweets = df[df['sentiment'] == 'negative']
n_perc = 100 * len(ntweets)/len(tweets)
print(f'Positive tweets {p_perc} %')
print(f'Neutral tweets {100 - p_perc - n_perc} %')
print(f'Negative tweets {n_perc} %')

#@title <- Grafica en WordCloud de los tweets

twt = " ".join(df['clean_tweets'])
wordcloud = WordCloud(stopwords=STOPWORDS, background_color='black', width=2500, height=2000).generate(twt)

plt.figure(1,figsize=(13, 13))
plt.imshow(wordcloud)
plt.axis('off')
plt.show()

#@title <- Cargue librerías

!pip3 install tweepy

#@title <- Generacion de datos del usuario de Twitter

import tweepy #https://github.com/tweepy/tweepy
import csv

#Credenciales del Twitter API
consumer_key = "MqqpoANH1PXbpl8Tq5Ys8WdP4"
consumer_secret = "vdjzTDWlgC13t1Cdl0j8gD3mQATD9kGbRC5mgWZxuqu5sols25"
access_key = "1298016354516140037-b4HAgP18N7fXAwJdjcebvUrlcTcGiD"
access_secret = "7tPLwRlJry3OeW1tN9XRblEE0pHqCmE3XfBIQtn7a6zm0"

#Remover los caracteres no imprimibles y los saltos de línea del texto del tweet
def strip_undesired_chars(tweet):
    stripped_tweet = tweet.replace('\n', ' ').replace('\r', '')
    char_list = [stripped_tweet[j] for j in range(len(stripped_tweet)) if ord(stripped_tweet[j]) in range(65536)]
    stripped_tweet=''
    for j in char_list:
        stripped_tweet=stripped_tweet+j
    return stripped_tweet

def get_all_tweets(screen_name):
    #Este método solo tiene permitido descargar máximo los ultimos 3240 tweets del usuario
    #Especificar aquí durante las pruebas un número entre 200 y 3240
    limit_number = 32
    
    #autorizar twitter, inicializar tweepy
    auth = tweepy.OAuthHandler(consumer_key, consumer_secret)
    auth.set_access_token(access_key, access_secret)
    api = tweepy.API(auth)
    
    #inicializar una list to para almacenar los Tweets descargados por tweepy
    alltweets = []    
    
    #Hacer una petición inicial por los 200 tweets más recientes (200 es el número máximo permitido)
    new_tweets = api.user_timeline(screen_name = screen_name,count=200)
    
    #guardar los tweets más recientes
    alltweets.extend(new_tweets)
    
    #guardar el ID del tweet más antiguo menos 1
    oldest = alltweets[-1].id - 1
    
    #recorrer todos los tweets en la cola hasta que no queden más
    while len(new_tweets) > 0 and len(alltweets) <= limit_number:
        print ("getting tweets before" + str(oldest))
        
        #en todas las peticiones siguientes usar el parámetro max_id para evitar duplicados
        new_tweets = api.user_timeline(screen_name = screen_name,count=200,max_id=oldest)
        
        #guardar los tweets descargados
        alltweets.extend(new_tweets)
        
        #actualizar el ID del tweet más antiguo menos 1
        oldest = alltweets[-1].id - 1
        
        #informar en la consola como vamos
        print (str(len(alltweets)) + " tweets descargados hasta el momento")
    
    #transformar los tweets descargados con tweepy en un arreglo 2D array que llenará el csv
    outtweets = [(tweet.id_str, tweet.created_at, strip_undesired_chars(tweet.text),tweet.retweet_count,str(tweet.favorite_count)+'') for tweet in alltweets]
    
    #escribir el csv    
    with open('%s_tweets.csv' % screen_name, "w", newline='') as f:       
        writer = csv.writer(f, quoting=csv.QUOTE_ALL)
        writer.writerow(['id','created_at','text','retweet_count','favorite_count'''])
        writer.writerows(outtweets)    
    pass

if __name__ == '__main__':
    #especificar el nombre de usuario de la cuenta a la cual se descargarán los tweets
    get_all_tweets(usuario)

#@title <- Presentar datos del usuario analizado

archivo_usser = "_tweets.csv"

df = pd.read_csv(usuario+archivo_usser)
df.text

#@title <- Generacion de archivos descargables
import pandas as pd
df = pd.read_csv(usuario+archivo_usser)

archivo_usser_xlsx = "_tweets.xlsx"

df.to_excel(usuario+archivo_usser_xlsx, index=False)

#@title <- Librerias de Excel
!pip install openpyxl

#@title <- Librerias de analisis y PLN

import spacy
import requests
from bs4 import BeautifulSoup

!pip install deplacy
!python -m spacy download es_core_news_sm
import pkg_resources,imp
imp.reload(pkg_resources)
import spacy
nlp=spacy.load("es_core_news_sm")
from collections import Counter
#!pip install textacy
#import textacy

import numpy as np
import matplotlib.pyplot as plt
!pip install monkeylearn

#@title <- Carga el nombre del archivo

import openpyxl
#archivoprevf = """x"""
#archivoprevf = """Flojo.xlsx"""
#doc = openpyxl.load_workbook ("Flojo.xlsx", read_only=True)

archivoprevf = """x"""
archivof = archivoprevf.replace("""x""", usuario+archivo_usser_xlsx)
doc = openpyxl.load_workbook (archivof , read_only=True)
hoja = doc.get_sheet_by_name("Sheet1")

#@title <- Categorizacion (Positivas / Neutral /Negativa)

tweets = fetch_tweets(query = usuario, count = 200)
# Converting the list into a pandas Dataframe
df = pd.DataFrame(tweets, columns= ['tweets', 'clean_tweets','sentiment'])

# Dropping the duplicate values just in case there are some tweets that are copied and then stores the data in a csv file
df = df.drop_duplicates(subset='clean_tweets')
df.to_csv(usuario+archivo_usser, index= False)
ptweets = df[df['sentiment'] == 'positive']
p_perc = 100 * len(ptweets)/len(tweets)
ntweets = df[df['sentiment'] == 'negative']
n_perc = 100 * len(ntweets)/len(tweets)
print(f'Positive tweets {p_perc} %')
print(f'Neutral tweets {100 - p_perc - n_perc} %')
print(f'Negative tweets {n_perc} %')

#@title <- Analisis y limpieza de la informacion

import spacy
import unicodedata


################### Arreglos ###################
respuestas=[]
palabras=[]
respuestasnone=[]
Respuestascompletas=[]
rta="tweet"

################### Información correspondiente al usuario desde hoja de calculo ###################
celdainicial = "C{0}"
celdainicialF = celdainicial.format(1)

celdafinal = "C{0}"
celdafinalF = celdafinal.format(1001)

multiple_cells = hoja[celdainicialF:celdafinalF]
for row in multiple_cells:
    for cell in row:
        respuestasnone.append(cell.value)
################### Limpieza de texto ###################
respuestas = list(filter(None, respuestasnone))
x = rta.join(respuestas)
output_clean = x.replace('\n', ' ')
output_clean2 = output_clean.replace('.', ' ')
output_cleanJuanito = output_clean2.replace('Https', ' ')
output_cleanJuanito = output_clean2.replace(':', ' ')
output_cleanJuanito = output_clean2.replace('/', ' ')
output_clean3 = output_cleanJuanito.replace(',', ' ')
trans_tab = dict.fromkeys(map(ord, u'\u0301\u0308'), None)#Tildes
output_clean4 = unicodedata.normalize('NFKC', unicodedata.normalize('NFKD', output_clean3).translate(trans_tab))#Formato de texto
output_clean5 = output_clean4.title() 
doc = nlp(output_clean5)
palabras = output_clean5.split()
################### Conteo de palabras ###################
contador = Counter(palabras)
buscar = 'depresion' 
import re
regex="("+".*?".join(buscar) + ")"
lista1Cadena=" ".join(palabras)
encontrados = re.finditer(regex, lista1Cadena, re.MULTILINE)
cantidad=len([matchNum for matchNum, match in enumerate(encontrados, start=1)])
print("____________________________________________")
print("___________YA SE PUEDEN EJECUTAR ___________")
print("__________LOS SIGUIENTES ANÁLISIS __________")
print("____________________________________________")
print("")

#@title <- Genera grafico de nube de palabras

wordcloud = WordCloud(stopwords=STOPWORDS, background_color='black', width=2500, height=2000).generate(lista1Cadena)

plt.figure(1,figsize=(13, 13))
plt.imshow(wordcloud)
plt.axis('off')
plt.show()

#@title <- Sujetos detectados
########################################################################SUJETOS###################################################################################################
fig = plt.figure(figsize=(10,5))
ax = fig.gca()

Esencial = [chunk.text for chunk in doc.noun_chunks]
contador = Counter(Esencial)

labels, values = zip(*contador.most_common(20))

indexes = np.arange(len(labels))
width = 1

ax.bar(indexes, values, 0.5)
ax.set_xticks(indexes)
ax.set_xticklabels( labels, rotation=90, fontsize=20)
ax.set_title("Sujetos detectados de las repuestas\ndadas por el paciente en la consulta.", fontsize=20)
plt.show()
########################

#@title <- Adjetivos detectados
##################################################################Adjetivos###############################################################################################
fig = plt.figure(figsize=(10,5))
ax = fig.gca()

ADJ_F = [] 

for token in doc:
  if token.pos_ == "ADJ":
     ADJ_F.append(token.text)

#print("Adjetivos = ",ADJ_F)
contador = Counter(ADJ_F)

labels, values = zip(*contador.most_common(20))

indexes = np.arange(len(labels))
width = 1

ax.bar(indexes, values, 0.5)
ax.set_xticks(indexes)
ax.set_xticklabels( labels, rotation=90, fontsize=20)
ax.set_title("Adjetivos detectados de las repuestas\ndadas por el paciente en la consulta.", fontsize=20)
plt.show()
####################################################################################################################################################################################

#@title <- Verbos detectados
##################################################################Verbos###############################################################################################
fig = plt.figure(figsize=(10,5))
ax = fig.gca()

VERB_F = [] 

for token in doc:
  if token.pos_ == "VERB":
     VERB_F.append(token.text)

#print("Verbos = ",VERB_F)
contador = Counter(VERB_F)

labels, values = zip(*contador.most_common(20))

indexes = np.arange(len(labels))
width = 1

ax.bar(indexes, values, 0.5)
ax.set_xticks(indexes)
ax.set_xticklabels( labels, rotation=90, fontsize=20)
ax.set_title("Verbos detectados de las repuestas\ndadas por el paciente en la consulta.", fontsize=20)
plt.show()
####################################################################################################################################################################################

#@title <- Palabras mas usadas
################################POR PALABRAS MAS COMUNES############################
fig = plt.figure(figsize=(10,5))
ax = fig.gca()

contador = Counter(palabras)

labels, values = zip(*contador.most_common(20))

indexes = np.arange(len(labels))
width = 1

ax.bar(indexes, values, 0.5)
ax.set_xticks(indexes)
ax.set_xticklabels( labels, rotation=90, fontsize=20)
ax.set_title("Las palabras más frecuentemente usadas\npor el paciente", fontsize=20)
plt.show()
############################################################

#@title <- Cuantas veces se encontro el concepto de Depresion:

print("El concepto de 'Depresion' se encontro {} vez en las respuestas".format(cantidad))
############################################################

#@title <- Opiniones de Twitter separado por comas

output_clean.replace(".",",")
output_clean.replace(";",",")
texto_x_comas = output_clean.split(',')

from itertools import islice

def group_elements(lst, chunk_size):
    lst = iter(lst)
    return iter(lambda: tuple(islice(lst, chunk_size)), ())

for new_list in group_elements(texto_x_comas , 1):
    print(new_list)

"""**Aca iria la relacion de la Ontologia de oraciones con las opiniones en frases de Twitter**"""

#@title <- Carga la Ontologia
Animo=["Perezoso","Desalentada","Desalentadas","Decaído","Desmotivado","Desalentado","Desalentados","Desalentó","No Tengo Ganas","No Quiero","Estoy Cansado","Me Canso","No Tengo Alientos","Estoy Cansada","Pena","Desaliento","Apenado","Penoso","Penosa","Apenada","Deprimido","Desanimado","Decaído","Melancólico","Me Hundo","Humillado","Depresivo","Depresión","Me Arrepiento","No Quería Hacerlo","Debía Hacerlo","Me Siento Mal","Estoy Deprimido","Me Deprime","Estoy Fatal","No Puedo Más","Me Duele","Dolor","Melancolía","Afligido","Afligida","Triste","Desconsolado","Sad","Emo","Nada","Tengo Un Vacío","Me Ahogo","Oscuridad","Estoy En El Fondo","Cayendo","Caída","Atrapado","Atrapada","No Tengo A Nadie","Tristeza","Abatimiento","Hábito","Igual","Hábitos","No Quiero Hacer","Perdida","Perdido","Pérdida","Túnel","Frágil","Miedo","Intranquilo","Desconsolado  ","Desilusionado ","Decepcionado","Desinteresado","Fatigado","Cansado","Intranquilo","Quebrantado","Ataque","Lloro","Libido","Lagrimoso","Falta","Sin Amor","Nada Me Sale Bien","Indigno","Me Siento Mal","Soledad","Estoy Solo","Solito","Me Siento ","Miserable","Nada Motivado","Desesperado","Desesperanzado","Reducido","Háptico","No Estoy Al Ciento Por Ciento","No Encuentro Calma","Muerte","Suicida","Plan Estructurado","Plan","Planeacion","Todo Es Mi Culpa","Soy El Culpable","Soy La Culpable","Por Mi Culpa","Soy Una Vergüenza","No Me Interesa","No Me Motiva","No Hay Nada","Abrumado","Desesperado","Desesperada","Abrumada","Me Hundo","No Soy Capaz","No Puedo","Todo Falla","No Hay Nada","Inhabilidad","Soy Invisible","Encerrado","Encerrada","Es Una Tortura","Paralizado","Desesperanza","Es Un Infierno","Peso En El Pecho","Peso","No Puedo Luchar ","Me Arrastra","Me Odio","Quiero Morir","Me Voy A Matar","No Quiero Despertar","No Quiero Seguir","Ojalá Me Muera","Culpa","Desinterés","Llorar","Perdida De Confianza","Solo","Miserable","Sensible","Pensamiento Recurrente","Desilusionado","Desesperanza","Desesperanzado","Decepcionado ","Fracasado","Contrariado","Desanimado ","Susceptible","Desaliento","Desalentado","Soló","Lloró","Desesperado","Melancólico","Insignificante","Triste","Vacío ","Irritable","Afligido ","Sad","Apagado","Down ","Nostálgico ","Decaído","Sensible","Deplorable ","Abatido","Inestable"]

Apatia=["No Tiro","No Me Dan Ganas","Pasivo","No Siento ","Nada De Sexo","No Quiero Nada","Dejadez","Dejado","Vacío","Abandono","Desinteresado","Desánimo","Desganado","Desaliento","Descuidado","No Quiero Hacer Nada","No Puedo","Me Pesa","Me Da Igual","Cansado","No Me Hace Nada","No Siento Lo Mismo","No Tiene Sentido","No Disfruto","No Me Interesa","Me Aislo","Es Una Agonía","Sin Ganas","No Siento Deseo","Indiferente","Nada Motivado","Desesperado","Desesperanzado","Reducido","Apatico","No Estoy Al Ciento Por Ciento","No Encuentro Calma","No Pasa","Jamás","Tampoco","Es Común Que No","Nadie","Ni Siquiera","Nada","No Quiero Hacer","Sin Amor","Amor Propio","No Puedo","Difícil","Me Queda Grande","Impotente","Inferioridad","Nada Me Sale Bien","Todo Me Sale Mal","Sensible","Nunca","Ninguno","Abatimiento","Inutilidad","Incapacidad","Aislado","Insatisfecho","Solo","Vacío","Ausente","Melancólico","Aislamiento","Ausente","Inferioridad","Desalentada","Desalentadas","Decaído","Desmotivado","Desalentado","Desalentados","Desalentó","Aislamiento","Desaliento","Desinteresado","Sin Interés ","Apático","Desmotivado","Invalidante","Insensible","Desapegado","Desagradecido","Despreocupado"]

Alimentacion=["Indiferente","Yo","Apetito","Desinterés","Emoción","Abatimiento","Dejadez","Dejado","Vacío","Abandono","Desinteresado","Desánimo","Deganado","Desaliento","Descuidado","Todo Yo","Siempre Es Igual","Siempre A Mi","Soy","No Como","Poco Como","Hambre","Vacío","Inapetente","No Tengo Hambre","Nada De Apetito","Desinteresado","Fatigado","Cansado","Intranquilo","Nada","No Quiero Comer","Impulso","Glotón","Goloso","Sediento","Glotona","Hastiado/A","Inapetente","Fastidiado","Repugnante","Vomito","Insasiable","Devoro","Lleno","Repleto","Hambriento","Antojado","Como Mucho","Ingiero","Pesadez ","Atracón"]

Descanso=["Débil","Débiles","Debilité","Raquítico","Debilitado","Enclenque","Flojo","Apagado","Debilitada","Mamado","Debilitados","Floja","Debilitadas","Desalentada","Desalentadas","Decaído","Desmotivado","Desalentado","Desalentados","Desalentó","Debilidad","Desaliento","Agitacion","Falta De Energia","Igual","Siempre","Lo Mismo","Dia A Dia","Rutinario","Todos Los Días","Cansado","Extenuado","Mamado","Harto","Igual","Hábitos","No Duermo","Duermo Mucho","Levanto Tarde","No Duermo","Insomnio","Desvelado","Desvelo","Sencillo","Sensible","Cotidiano","Extenuación","Hábito","Sueño","Susceptible","Nada Motivado","Desesperado","Desesperanzado","Reducido","Háptico","No Estoy Al Ciento Por Ciento","No Encuentro Calma","Sensible","Agotado","Sin Ganas","Cansado","Cansada","Exhausto","Exhausta","Somnoliento","Molido","Rendido ","Saturado","Debilitado","Pesado","Desvanecido","Acabado","Cargado","Cansado"]

Actividad_fisica=["Desaliento","Frustración","Trauma","Abatimiento","Extenuación","Incapacidad","Agitado","Débil","Débiles","Debilité","Raquítico","Debilitado","Enclenque","Flojo","Apagado","Debilitada","Mamado","Debilitados","Floja","Debilitadas","Desalentada","Desalentadas","Decaído","Desmotivado","Desalentado","Desalentados","Desalentó","Frustrado","Fracasado","Fiasco","Chasco","Traumatizado","Afectado","Marcado","Encadenado","Enyugado","Atado","Nada","No Quiero Hacer","Cansado","Extenuado","Mamado","Harto","No Puedo","Difícil","Me Queda Grande","Impotente","Nerviosismo","Ansioso","Malo","Malestar","Inmunda","Inquieto","Preocupado","Preocupación","Debilidad","Lento","Inquieto","Pasivo","Pausado","Fatigado","Acelerado","Inerte","Activo","Flojo","Tonto","Retardado","Revuelto","Pasmado","Callado"]

Desaliento=["Ausente","Ausentes","Ausentado","Ausencia","Alejado","Lejano","Ausentada","Ausentó","Ausentará","Ausente","Débil","Débiles","Debilité","Raquítico","Debilitado","Enclenque","Flojo","Apagado","Debilitada","Mamado","Debilidad","Floja","Desalentada","Decaído","Desmotivado","Desalentado","Desaliento","Desalentó","Errores","Embarrarla","Cagarla","Tirarla","Surrarla","Miedo Al Fracaso","Derrotado","Abatido","Cabizbajo","Desanimado","Melancólico","Decaído","Derrumbado","Desmoronado","Abatimiento","Igual","Siempre","Repetitivo","Tedioso","Rutinario","Rutina ","Cansado","Extenuado","Mamado","Harto","Monotonía","Cotidiano","Extenuación","Hábito","Hábitos","Hábito","Desvelo","Somnolente","Adormecido","Somnolento","Insomnio","Desvelado","Desvelo","Sencillo","Sensible","Afligido","Amargura","Amargado","Enfadado","Molesto","Desinteresado","Fatigado","Cansado","Intranquilo","Sueño","Susceptible","Aflicción","Desinterés","Apático","Indiferente","Aislado","Insatisfecho","Solo","Vacío","Ausente","Melancólico","Negativo","Distraído","Problemas","Disperso","Elevado","Aislamiento","Atención","Nerviosismo","Quebrantado","Inmunda","Inquieto","Inunda","Desfallecido","Nervioso","Cargado","Aburrido","Agobiado","Sofocado","Inatento","Desatento","Aburrimiento","Agobiado","Pesado","Agotado","Debil ","Agotada","Dejado","Decaido ","Decaida","Fragil","Molido","Desmayado","Asfixiado","Agobiado","Desganado ","Distante","Enfermo","Parsimonioso","Sueño"]

Cargo_de_conciencia=["Culpable","Culpado","Culpada","Culpa","Culpé","Culpar","Culpables","Errores","Embarrarla","Cagarla","Tirarla","Surrarla","Frustrado","Fracasado","Fiasco","Chasco","Apenado","Penoso","Penosa","Apenada","Deprimido","Desanimado","Decaído","Melancólico","Hundido","Depresivo","Lamentar","Lamentó","Lamenté","Lamento","Afligido","Afligida","Arrepentido","Arrepentida","Arrepentidos","Arrepentidas","Arrepintieron","Arrepentí","Desmoronado","No Quiero Hacer","Confuso","Confundo","Incomprender","Perdido","Desvelo","Somnolente","Adormecido","Somnolento","Insomnio","Desvelado","Desvelo","Vulnerable","Sensible","Frágil","Afligido","Amargura","Amargado","Enfadado","Molesto","Miedo","Intranquilo","Desconsolado  ","Desilusionado ","Decepcionado","Intranquilo","Quebrantado","Ataque","Lloro","Libido","Lagrimoso","Aislado","Insatisfecho","Solo","Vacío","Ausente","Melancólico","Negativo","Autocriticar","Indigno","Inmunda","Preocupado","Preocupación","Dudo","Dudar","Incierto","Dejadez","Dejado","Vacío","Abandono","Desinteresado","Desánimo","Desganado","Desaliento","Descuidado","Futuro","Irresponsabilidad","Irresponsable","Responsable","Responsabilidad","Incumplido","Aplazar","Procrastino","Evadir","Evitar","Rumiar","Rumiación","Incapz","Inútil","Torpe","Inepto","Incompetente","Inhábil","Ignorante","Insuficiente","Inseguro","Remordimiento","Acusado","Derrotado","Derrumbado","Desorrientado","Cabizbajo","Desanimado","Melancólico","Decaído","Irritable","Culpabilidad","Miedo Al Fracaso","Frustración","Pena","Tristeza","Acusación","Abatimiento","Confusión","Sueño","Susceptible","Aflicción","Angustia","Irritable","Llorar","Aislamiento","Autoestima","Nervioso","Duda","Indiferente","Inservible","Incompetente","Fracasado","Incapaz","Desastroso","Culpable","Desilusionado","Irresponsable","Preocupado","Preocupada","Inseguro","Inutil","Insuficiente","Quejambroso","Reproche","Inferoridad","Inferior ","Incomprendido ","Mediocre","Decepcionado","Decepcionada","Tonto","Vago"]

Atencion_dispersa=["Ausente","Ausentes","Ausentado","Ausentados","Ausentada","Ausentadas","Ausentó","Ausentará","Ausente","Ausencia","Errores","Embarrarla","Cagarla","Tirarla","Cerrarla","Frustrado","Fracasado","Fiasco","Chasco","Frustración","Disperso","Traumatizado","Afectado","Marcado","Encadenado","Enyugado","Atado","Trauma","No Duermo","Duermo Mucho","Levanto Tarde","No Duermo","Insomnio","Desvelado","Sueño","Distraído","Problemas","No Concentrado","Dudo","Dudar","Incierto","Cargado","Duda","Confuso","Confundo","No Entiendo","Perdido","Bruto","Confusión","Confundido","Confundida","Distraído ","Desatento","Olvidadizo ","Incoherente","Descuidado","Embolatado","Desconcentrado","Despistado","Desordenado","Desmemoriado ","Equivocado","Desorientado","Descoordinado","Torpe","Ido","Englobado"]

Ideas_suicidas=["Desvelo","Sensible","Frágil","Miedo","Intranquilo","Desconsolado  ","Desilusionado ","Decepcionado","Susceptible","Sosobra","Aislado","Insatisfecho","Solo","Vacío","Ausente","Melancólico","Negativo","Aislamiento","Me Siento ","Miserable","Nada Motivado","Desesperado","Desesperanzado","Reducido","Apatico","No Estoy Al Ciento Por Ciento","No Encuentro Calma","Sensible","Tristeza","Melancolía","Afligido","Afligida","Triste","Desconsolado","Sad","Emo","Estorbo","Basura","Tortura","Dolor","Problema","Muero","Morir","Muerte","Desaparecer","Harto","Suicidarme","Suicidio","Matarme","Desaparecer","Down","Sin Futuro","Morir ","Perdido","Destruido","Aliviar Dolor","Golpearme","Sin Esperanza","Dolor","Muerte ","Llegar Al Final","Colgarme","Lanzarme","Derrotado","Rendido","Impotente","Desbordado ","Desesperado","Desesperada","Quebrantado","Sufrimiento","Hacerme Daño"]

#@title <- Relaciona la Ontologia de sintomatologia depresvia
ramaf = []

print("Palabras relacionadas con la sintomatologia: Animo")
rama = []
palabras_estudiar=set(palabras)
Animo=set(Animo)
Animo= palabras_estudiar & Animo
if len(Animo) > 0 :
    print("Hay {} elementos coincidentes".format(len(Animo)))
    print(Animo)
    rama.append("Animo")
    ramaf.append("Animo")
else:
    print("No hay repeticiones")
print("")
######

print("Palabras relacionadas con la sintomatologia: Apatia")
rama = []
palabras_estudiar=set(palabras)
Apatia=set(Apatia)
Apatia= palabras_estudiar & Apatia
if len(Apatia) > 0 :
    print("Hay {} elementos coincidentes".format(len(Apatia)))
    print(Apatia)
    rama.append("Apatia")
    ramaf.append("Apatia")
else:
    print("No hay repeticiones")
print("")
######

print("Palabras relacionadas con la sintomatologia: Alimentacion")
rama = []
palabras_estudiar=set(palabras)
Alimentacion=set(Alimentacion)
Alimentacion= palabras_estudiar & Alimentacion
if len(Alimentacion) > 0 :
    print("Hay {} elementos coincidentes".format(len(Alimentacion)))
    print(Alimentacion)
    rama.append("Alimentacion")
    ramaf.append("Alimentacion")
else:
    print("No hay repeticiones")
print("")   
######

print("Palabras relacionadas con la sintomatologia: Descanso")
rama = []
palabras_estudiar=set(palabras)
Descanso=set(Descanso)
Descanso= palabras_estudiar & Descanso
if len(Descanso) > 0 :
    print("Hay {} elementos coincidentes".format(len(Descanso)))
    print(Descanso)
    rama.append("Descanso")
    ramaf.append("Descanso")
else:
    print("No hay repeticiones")
print("")   
######

print("Palabras relacionadas con la sintomatologia: Actividad_fisica")
rama = []
palabras_estudiar=set(palabras)
Actividad_fisica=set(Actividad_fisica)
Actividad_fisica= palabras_estudiar & Actividad_fisica
if len(Actividad_fisica) > 0 :
    print("Hay {} elementos coincidentes".format(len(Actividad_fisica)))
    print(Actividad_fisica)
    rama.append("Actividad_fisica")
    ramaf.append("Actividad_fisica")
else:
    print("No hay repeticiones")
print("")        
######

print("Palabras relacionadas con la sintomatologia: Desaliento")
rama = []
palabras_estudiar=set(palabras)
Desaliento=set(Desaliento)
Desaliento= palabras_estudiar & Desaliento
if len(Desaliento) > 0 :
    print("Hay {} elementos coincidentes".format(len(Desaliento)))
    print(Desaliento)
    rama.append("Desaliento")
    ramaf.append("Desaliento")
else:
    print("No hay repeticiones")
print("")            
######

print("Palabras relacionadas con la sintomatologia: Cargo_de_conciencia")
rama = []
palabras_estudiar=set(palabras)
Cargo_de_conciencia=set(Cargo_de_conciencia)
Cargo_de_conciencia = palabras_estudiar & Cargo_de_conciencia
if len(Cargo_de_conciencia) > 0 :
    print("Hay {} elementos coincidentes".format(len(Cargo_de_conciencia)))
    print(Cargo_de_conciencia)
    rama.append("Cargo_de_conciencia")
    ramaf.append("Cargo_de_conciencia")
else:
    print("No hay repeticiones")
print("")                
######

print("Palabras relacionadas con la sintomatologia: Atencion_dispersa")
rama = []
palabras_estudiar=set(palabras)
Atencion_dispersa=set(Atencion_dispersa)
Atencion_dispersa = palabras_estudiar & Atencion_dispersa
if len(Atencion_dispersa) > 0 :
    print("Hay {} elementos coincidentes".format(len(Atencion_dispersa)))
    print(Atencion_dispersa)
    rama.append("Atencion_dispersa")
    ramaf.append("Atencion_dispersa")
else:
    print("No hay repeticiones")
print("")                    
######

print("Palabras relacionadas con la sintomatologia: Ideas_suicidas")
rama = []
palabras_estudiar=set(palabras)
Ideas_suicidas=set(Ideas_suicidas)
Ideas_suicidas = palabras_estudiar & Ideas_suicidas
if len(Ideas_suicidas) > 0 :
    print("Hay {} elementos coincidentes".format(len(Ideas_suicidas)))
    print(Ideas_suicidas)
    rama.append("Ideas_suicidas")
    ramaf.append("Ideas_suicidas")
else:
    print("No hay repeticiones")

#@title <- WorldCloud "Animo"

ramaf = []

print("Palabras relacionadas con la sintomatologia: Animo")
rama = []
palabras_estudiar=set(palabras)
Animo=set(Animo)
Animo= palabras_estudiar & Animo
if len(Animo) > 0 :
    print("Hay {} elementos coincidentes".format(len(Animo)))
    print(Animo)
    rama.append("Animo")
    ramaf.append("Animo")
else:
    print("No hay repeticiones")
print("")

listaAnimo = palabras_estudiar & Animo
StrAnimo = list(listaAnimo)
WorldCloudAnimo = " ".join(StrAnimo)

wordcloud = WordCloud(stopwords=STOPWORDS, background_color='black', width=2500, height=2000).generate(WorldCloudAnimo)

plt.figure(1,figsize=(13, 13))
plt.imshow(wordcloud)
plt.axis('off')
plt.show()

#@title <- WorldCloud "Apatia"

ramaf = []

print("Palabras relacionadas con la sintomatologia: Apatia")
rama = []
palabras_estudiar=set(palabras)
Apatia=set(Apatia)
Apatia= palabras_estudiar & Apatia
if len(Apatia) > 0 :
    print("Hay {} elementos coincidentes".format(len(Apatia)))
    print(Apatia)
    rama.append("Apatia")
    ramaf.append("Apatia")
else:
    print("No hay repeticiones")
print("")

listaApatia = palabras_estudiar & Apatia
StrApatia = list(listaApatia)
WorldCloudApatia = " ".join(StrApatia)

wordcloud = WordCloud(stopwords=STOPWORDS, background_color='black', width=2500, height=2000).generate(WorldCloudApatia)

plt.figure(1,figsize=(13, 13))
plt.imshow(wordcloud)
plt.axis('off')
plt.show()

#@title <- WorldCloud "Alimentacion"

ramaf = []

print("Palabras relacionadas con la sintomatologia: Alimentacion")
rama = []
palabras_estudiar=set(palabras)
Alimentacion=set(Alimentacion)
Alimentacion= palabras_estudiar & Alimentacion
if len(Alimentacion) > 0 :
    print("Hay {} elementos coincidentes".format(len(Alimentacion)))
    print(Alimentacion)
    rama.append("Alimentacion")
    ramaf.append("Alimentacion")
else:
    print("No hay repeticiones")
print("")   

listaAlimentacion = palabras_estudiar & Alimentacion
StrAlimentacion = list(listaAlimentacion)
WorldCloudAlimentacion = " ".join(StrAlimentacion)

wordcloud = WordCloud(stopwords=STOPWORDS, background_color='black', width=2500, height=2000).generate(WorldCloudAlimentacion)

plt.figure(1,figsize=(13, 13))
plt.imshow(wordcloud)
plt.axis('off')
plt.show()

#@title <- WorldCloud "Descanso"

ramaf = []

print("Palabras relacionadas con la sintomatologia: Descanso")
rama = []
palabras_estudiar=set(palabras)
Descanso=set(Descanso)
Descanso= palabras_estudiar & Descanso
if len(Descanso) > 0 :
    print("Hay {} elementos coincidentes".format(len(Descanso)))
    print(Descanso)
    rama.append("Descanso")
    ramaf.append("Descanso")
else:
    print("No hay repeticiones")
print("")   


listaDescanso = palabras_estudiar & Descanso
StrDescanso = list(listaDescanso)
WorldCloudDescanso = " ".join(StrDescanso)

wordcloud = WordCloud(stopwords=STOPWORDS, background_color='black', width=2500, height=2000).generate(WorldCloudDescanso)

plt.figure(1,figsize=(13, 13))
plt.imshow(wordcloud)
plt.axis('off')
plt.show()

#@title <- WorldCloud "Actividad_fisica"

ramaf = []

print("Palabras relacionadas con la sintomatologia: Actividad_fisica")
rama = []
palabras_estudiar=set(palabras)
Actividad_fisica=set(Actividad_fisica)
Actividad_fisica= palabras_estudiar & Actividad_fisica
if len(Actividad_fisica) > 0 :
    print("Hay {} elementos coincidentes".format(len(Actividad_fisica)))
    print(Actividad_fisica)
    rama.append("Actividad_fisica")
    ramaf.append("Actividad_fisica")
else:
    print("No hay repeticiones")
print("")    


listaActividad_fisica = palabras_estudiar & Actividad_fisica
StrActividad_fisica = list(listaActividad_fisica)
WorldCloudActividad_fisica = " ".join(StrActividad_fisica)

wordcloud = WordCloud(stopwords=STOPWORDS, background_color='black', width=2500, height=2000).generate(WorldCloudActividad_fisica)

plt.figure(1,figsize=(13, 13))
plt.imshow(wordcloud)
plt.axis('off')
plt.show()

#@title <- WorldCloud "Desaliento"

ramaf = []

print("Palabras relacionadas con la sintomatologia: Desaliento")
rama = []
palabras_estudiar=set(palabras)
Desaliento=set(Desaliento)
Desaliento= palabras_estudiar & Desaliento
if len(Desaliento) > 0 :
    print("Hay {} elementos coincidentes".format(len(Desaliento)))
    print(Desaliento)
    rama.append("Desaliento")
    ramaf.append("Desaliento")
else:
    print("No hay repeticiones")
print("")        


listaDesaliento = palabras_estudiar & Desaliento
StrDesaliento = list(listaDesaliento)
WorldCloudDesaliento = " ".join(StrDesaliento)

wordcloud = WordCloud(stopwords=STOPWORDS, background_color='black', width=2500, height=2000).generate(WorldCloudDesaliento)

plt.figure(1,figsize=(13, 13))
plt.imshow(wordcloud)
plt.axis('off')
plt.show()

#@title <- WorldCloud "Cargo_de_conciencia"

ramaf = []

print("Palabras relacionadas con la sintomatologia: Cargo_de_conciencia")
rama = []
palabras_estudiar=set(palabras)
Cargo_de_conciencia=set(Cargo_de_conciencia)
Cargo_de_conciencia = palabras_estudiar & Cargo_de_conciencia
if len(Cargo_de_conciencia) > 0 :
    print("Hay {} elementos coincidentes".format(len(Cargo_de_conciencia)))
    print(Cargo_de_conciencia)
    rama.append("Cargo_de_conciencia")
    ramaf.append("Cargo_de_conciencia")
else:
    print("No hay repeticiones")
print("")        


listaCargo_de_conciencia = palabras_estudiar & Cargo_de_conciencia
StrCargo_de_conciencia = list(listaCargo_de_conciencia)
WorldCloudCargo_de_conciencia = " ".join(StrCargo_de_conciencia)

wordcloud = WordCloud(stopwords=STOPWORDS, background_color='black', width=2500, height=2000).generate(WorldCloudCargo_de_conciencia)

plt.figure(1,figsize=(13, 13))
plt.imshow(wordcloud)
plt.axis('off')
plt.show()

#@title <- WorldCloud "Atencion_dispersa"

ramaf = []

print("Palabras relacionadas con la sintomatologia: Atencion_dispersa")
rama = []
palabras_estudiar=set(palabras)
Atencion_dispersa=set(Atencion_dispersa)
Atencion_dispersa = palabras_estudiar & Atencion_dispersa
if len(Atencion_dispersa) > 0 :
    print("Hay {} elementos coincidentes".format(len(Atencion_dispersa)))
    print(Atencion_dispersa)
    rama.append("Atencion_dispersa")
    ramaf.append("Atencion_dispersa")
else:
    print("No hay repeticiones")
print("")     


listaAtencion_dispersa = palabras_estudiar & Atencion_dispersa
StrAtencion_dispersa = list(listaAtencion_dispersa)
WorldCloudAtencion_dispersa = " ".join(StrAtencion_dispersa)

wordcloud = WordCloud(stopwords=STOPWORDS, background_color='black', width=2500, height=2000).generate(WorldCloudAtencion_dispersa)

plt.figure(1,figsize=(13, 13))
plt.imshow(wordcloud)
plt.axis('off')
plt.show()

#@title <- WorldCloud "Ideas_suicidas"

ramaf = []

print("Palabras relacionadas con la sintomatologia: Ideas_suicidas")
rama = []
palabras_estudiar=set(palabras)
Ideas_suicidas=set(Ideas_suicidas)
Ideas_suicidas = palabras_estudiar & Ideas_suicidas
if len(Ideas_suicidas) > 0 :
    print("Hay {} elementos coincidentes".format(len(Ideas_suicidas)))
    print(Ideas_suicidas)
    rama.append("Ideas_suicidas")
    ramaf.append("Ideas_suicidas")
else:
    print("No hay repeticiones")  


listaIdeas_suicidas = palabras_estudiar & Ideas_suicidas
StrIdeas_suicidas = list(listaIdeas_suicidas)
WorldCloudIdeas_suicidas = " ".join(StrIdeas_suicidas)

wordcloud = WordCloud(stopwords=STOPWORDS, background_color='black', width=2500, height=2000).generate(WorldCloudIdeas_suicidas)

plt.figure(1,figsize=(13, 13))
plt.imshow(wordcloud)
plt.axis('off')
plt.show()

#@title <- Genera informe de resultados
#cargar informe

contador = Counter(palabras)

mas = contador.most_common(20)

Generar_informe=open("InformeTwitter.txt","w") 
Generar_informe.close()

informe = ("Este es el informe generado para el paciente con el ID : {0} \n"
           "\n"
           "El escribió estas palabras en sus respuestas: {1} \n"
           "\n"
           "Al relacionar estas palabras con la Ontología encontramos que presenta correlación con: \n"
           "La sintomatologia de : {2}"
           "\n"
           "Estos son los conceptos identifacados por rama especifica:"
           "\n"
           "Animo: {3}"
           "\n"
           "Apatia: {4} "
           "\n"
           "Alimentacion: {5}"
           "\n"
           "Descanso: {6}"
           "\n"
           "Actividad_fisica: {7}"
           "\n"
           "Desaliento: {8}"
           "\n"
           "Cargo_de_conciencia: {9}"
           "\n"
           "Atencion_dispersa: {10}"
           "\n"
           "Ideas_suicidas: {11}"
           "\n"
           "Si anteriormente no se ve ninguna relación es porque no se encontraron correlaciones.\n"
           "\n"
           "Las palabras mas significativas dadas por el consultante son: \n"
           "{12}"
           "\n"
           "\nLos adjetivos que este utilizo fueron: \n"
           "{13}"
           "\n"
           "\nLos verbos que este utilizo fueron: \n"
           "{14}"
           "\n"
           "\nLas palabras mas usadas por el consultante son: \n"
           "{15}"
           "\n"
           "\n"
           "\n"
           "\nSi en alguno de los anteriores, te genero vacío, es porque el consultante no utilizo ningún concepto\n"
           "relacionado a esa área especifica.\n"
           )

informeP = informe.format(id,palabras,ramaf,Animo,Apatia,Alimentacion,Descanso,Actividad_fisica,Desaliento,Cargo_de_conciencia,Atencion_dispersa,Ideas_suicidas,Esencial,ADJ_F,VERB_F,mas)
informeF = "".join(informeP)

Cargar_informe=open("InformeTwitter.txt","a")
Cargar_informe.write(informeF)
Cargar_informe.close()